---
layout: post
title: 深度学习小结
categories: 17th
description: 小结
keywords: 深度学习

---

因为最近刚好要对深度学习做个报告，这里做个小结。基本知识网上一搜都能找到，我只按着自己的理解从下面三个部分来说明神经网络的一些通性。

## 发展溯源

深度学习发展到今天，经过一波三折，追溯它的起源其实是对它的原理，背景以及未来的发展做些了解，从而更好地去理解神经网络演变到深度学习，这些算法之间的通性。

先来看一张图：

![1](http://upload-images.jianshu.io/upload_images/2360187-6b979a96c9119315.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

图中可见三个深度学习发展的低谷，同时也是兴起的转折点，先来看1943年的MCP人工神经元模型。

![2](http://upload-images.jianshu.io/upload_images/2360187-2a0ef47c0c193cbf?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这个构架就已经涵盖了现今很多神经网络的特点，权重与输入的乘积之和，通过激活函数输出等等，1958年Rosenblatt用它发明了单层感知器，但1969年感知器被Minsky证明本质是线性模型，发展就此搁浅。

到1986年Hinton发明了BP神经网络，采用Sigmoid函数进行非线性映射，有效解决非线性分类的问题，但是在1991年BP算法被指出有梯度消失的问题，陷入冰点。

2006年Hinton提出了深层网络训练中梯度消失问题的解决方案：无监督预训练对权值进行初始化+有监督训练微调。至此，深度学习就不断地引起人们的重视，蓬勃发展至今。[参考](http://blog.csdn.net/u012177034/article/details/52252851)

## 每一层的输入输出

先来看几个网络。

###BP神经网络

![3](http://upload-images.jianshu.io/upload_images/2360187-6cb1cad360aad7f6.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

可以看到，输入是上一层输出的值分别与权重相乘的和，而输出是输入通过激活函数变化后的值，BP中常用的激活函数是Sigmoid函数。

###深度信念网络（DBNs）

![4](http://upload-images.jianshu.io/upload_images/2360187-f0bfd4f6fce43932.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这个网络是由多层的受限玻尔兹曼机（RBM）组成，多层的无监督学习和顶层的有监督学习。是为了解决BP神经网络中出现的梯度消失的问题而出现的，从之前的发展历程中也可以看到。

同样的，它的输入输入输出和BP神经网络是一样的，不过激活函数的选择可以是不一样的，不同的是它的训练机制，我们下面再讲。

###卷积神经网络（CNN）

卷积神经网络适合于用作图像识别分类上，输入是一个（p*p*3）的矩阵，p的大小由图像的像素大小决定，矩阵上的每一个值都是0-255之间的值，熟悉rgb三色通道的同志们应该了解这个，这也解释了矩阵厚度为什么为3。

至于输出，输入矩阵通过滤波器来对图像的特征进行提取，不同的滤波器得到的特征不同，得到的输出也就不同，看下图。

![5](http://upload-images.jianshu.io/upload_images/2360187-22d71ae8b1a9f3cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

图中两个不同的滤波器，得到了两个不同的矩阵（绿色），其实这种从高维图像中提取低维信息的过程也就是卷积的目的。[参考1](https://deeplearning4j.org/cn/convolutionalnets)[参考2](https://www.wikiwand.com/zh-cn/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)

## 如何训练

关于训练过程，有一个损失函数的概念，BP要使损失函数尽量小，那么需对损失函数求导，求极值，这里采用梯度下降的方式调整权重。但是会有局部极小值和梯度消失的问题。

而深度信念网络，在受限玻尔兹曼机无标签训练的过程中，是希望每一层前向传播的概率值和后向传播的概率值尽量相等。可能难以理解，看张图：

![6](http://upload-images.jianshu.io/upload_images/2360187-ad5d6a808e649aec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

 >在重构阶段，第一隐藏层的激活值成为反向传递中的输入。这些输入值与同样的权重相乘，每两个相连的节点之间各有一个权重，就像正向传递中输入x的加权运算一样。这些乘积的和再与每个可见层的偏差相加，所得结果就是重构值，亦即原始输入的近似值。由于RBM权重初始值是随机决定的，重构值与原始输入之间的差别通常很大。可以将r值与输入值之差视为重构误差，此误差值随后经由反向传播来修正RBM的权重，如此不断反复，直至误差达到最小。

经过无监督训练之后，再通过有监督的训练队全局的权重进行调优。[参考](https://deeplearning4j.org/cn/restrictedboltzmannmachine)

卷积神经网络的训练也是一个是损失函数最小的过程，但是具体的话我还不理解。以后有机会再来写。

## 最后

深度学习深究起来真是要掉不少头发，网络安全领域，我还是希望只把它当成数据分析的工具用，比方说深度学习态势感知，恶意代码检测。
还是先学会用吧，转化成有价值的东西，总比空想强。